{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN basics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "1. Hidden state\n",
    "2. Input\n",
    "3. RNN layer\n",
    "4. Send data to RNN\n",
    "5. Word embedding using nn.Embedding\n",
    "6. Word embedding using nn.Linear\n",
    "7. Testing Linear layer - one hot encoding\n",
    "8. Testing Linear layer - Integer\n",
    "9. Testing embedding\n",
    "10. Linear layer- one hot encoding - batch size 1\n",
    "11. RNN score computation - one forward pass\n",
    "12. Bidirectional RNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden state\n",
    "\n",
    "h_0 of shape (num_layers * num_directions, batch, hidden_size): \n",
    "\n",
    "tensor containing the initial hidden state for each element in the batch. \n",
    "\n",
    "Defaults to zero if not provided. \n",
    "\n",
    "If the RNN is bidirectional, num_directions should be 2, else it should be 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 1\n",
    "num_directions = 1\n",
    "batch_size = 2\n",
    "hidden_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_init = torch.zeros (num_layers * num_directions, batch_size, hidden_size)\n",
    "print(h_init)\n",
    "print(h_init.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input\n",
    "\n",
    "Input of shape (seq_len, batch, input_size): \n",
    "\n",
    "tensor containing the features of the input sequence. \n",
    "\n",
    "The input can also be a packed variable length sequence.\n",
    "\n",
    "seq_length = along row, batch size = along column, input_size = size of the word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 2\n",
    "batch_size = 2\n",
    "input_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.rand(seq_length,batch_size,input_size)\n",
    "print(input)\n",
    "print(input.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN layer\n",
    "\n",
    " h_t = tanh(W_{ih} x_t + b_{ih} + W_{hh} h_{(t-1)} + b_{hh})\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.RNN.html\n",
    "\n",
    "nonlinearity – The non-linearity to use. Can be either 'tanh' or 'relu'. Default: 'tanh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 3\n",
    "hidden_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.RNN(input_size,hidden_size,nonlinearity = 'tanh',bias = True)\n",
    "print(rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Send data to RNN\n",
    "\n",
    "output of shape (seq_len, batch, num_directions * hidden_size): tensor containing the output features (h_t) from the last layer of the RNN, for each t. If a torch.nn.utils.rnn.PackedSequence has been given as the input, the output will also be a packed sequence.\n",
    "\n",
    "For the unpacked case, the directions can be separated using output.view(seq_len, batch, num_directions, hidden_size), with forward and backward being direction 0 and 1 respectively. Similarly, the directions can be separated in the packed case.\n",
    "\n",
    "h_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_seq, h_final = rnn(input,h_init)\n",
    "print(h_seq)\n",
    "print(h_final)\n",
    "print(h_seq.size(),\"\\n\", h_final.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embedding using nn.Embedding\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
    "\n",
    "num_embeddings (int) – size of the dictionary of embeddings\n",
    "\n",
    "embedding_dim (int) – the size of each embedding vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_embeddings = 6\n",
    "embedding_dim = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 3\n",
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight Parameter containing:\n",
      "tensor([[-0.5925, -0.8228,  0.2863,  0.8048,  0.1445],\n",
      "        [-0.4658,  0.8823, -0.2044, -1.0068,  0.1941],\n",
      "        [ 1.4252,  0.2355, -0.9920, -0.3669,  0.3220],\n",
      "        [ 1.4867,  0.3734,  0.1834, -2.3481, -0.7169],\n",
      "        [-0.3742, -2.8740, -0.3611,  0.1126, -0.1899],\n",
      "        [-1.6323,  0.2826,  0.8102,  0.4583,  0.0803]], requires_grad=True)\n",
      "input tensor([[1, 2],\n",
      "        [3, 3],\n",
      "        [5, 5]])\n",
      "tensor([[[-0.4658,  0.8823, -0.2044, -1.0068,  0.1941],\n",
      "         [ 1.4252,  0.2355, -0.9920, -0.3669,  0.3220]],\n",
      "\n",
      "        [[ 1.4867,  0.3734,  0.1834, -2.3481, -0.7169],\n",
      "         [ 1.4867,  0.3734,  0.1834, -2.3481, -0.7169]],\n",
      "\n",
      "        [[-1.6323,  0.2826,  0.8102,  0.4583,  0.0803],\n",
      "         [-1.6323,  0.2826,  0.8102,  0.4583,  0.0803]]],\n",
      "       grad_fn=<EmbeddingBackward>)\n",
      "torch.Size([3, 2, 5])\n"
     ]
    }
   ],
   "source": [
    "w_emb = nn.Embedding(num_embeddings, embedding_dim)\n",
    "print(\"weight\",w_emb.weight)\n",
    "input = torch.LongTensor([[1,2,3],[3,5,5]]).view(seq_length,batch_size)\n",
    "print(\"input\",input)\n",
    "output = w_emb(input)\n",
    "print(output)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embedding using nn.Linear\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.Linear.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_features = num_embeddings\n",
    "out_features = embedding_dim\n",
    "#in_features =5\n",
    "#out_features = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = nn.Linear(in_features, out_features, bias = False)\n",
    "print(linear)\n",
    "print(linear.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seq_length = 1\n",
    "#batch_size = 2\n",
    "input_size = out_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "batch_1 = torch.Tensor([[1,0,0,0],[0,0,1,0],[0,0,0,1]])\n",
    "batch_2 = torch.Tensor([[0,0,1,0],[0,1,0,0],[1,0,0,0]])\n",
    "\n",
    "sequence = torch.cat((batch_1,batch_2),dim = 0).view(seq_length,batch_size,in_features)\n",
    "print(sequence)\n",
    "print(sequence.size())\n",
    "#print(inputs.size())\n",
    "output = linear(sequence)\n",
    "print(output.size())\n",
    "#output = output.view(seq_length,batch_size,input_size)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing - Linear layer - One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear layer\n",
    "\n",
    "in_features = 4\n",
    "out_features = 5\n",
    "seq_length = 3\n",
    "batch_size = 2\n",
    "\n",
    "# RNN layer\n",
    "\n",
    "num_layers = 1\n",
    "num_directions = 1\n",
    "hidden_size = 5\n",
    "input_size = out_features\n",
    "\n",
    "# Linear layer\n",
    "linear = nn.Linear(in_features, out_features, bias = False)\n",
    "print(linear)\n",
    "print(linear.weight)\n",
    "\n",
    "# one hot encoding\n",
    "\n",
    "batch_1 = torch.Tensor([[1,0,0,0],[0,0,1,0],[0,0,0,1]])\n",
    "batch_2 = torch.Tensor([[0,0,1,0],[0,1,0,0],[1,0,0,0]])\n",
    "\n",
    "input_data = torch.cat((batch_1,batch_2),dim = 0).view(seq_length,batch_size,in_features)\n",
    "print(\"input_data\",input_data)\n",
    "print(\"Size of the input data\",input_data.size())\n",
    "\n",
    "output = linear(input_data)\n",
    "print(\"Size of the Linear layer output\",output.size())\n",
    "#output = output.view(seq_length,batch_size,input_size)\n",
    "print(\"linear layer output\", output)\n",
    "\n",
    "# Memory state\n",
    "h_init = torch.zeros (num_layers * num_directions, batch_size, hidden_size)\n",
    "print(\"Memory state initialisation\", h_init)\n",
    "print(\"Memory state size\",h_init.size())\n",
    "\n",
    "rnn = nn.RNN(input_size,hidden_size,nonlinearity = 'tanh',bias = True)\n",
    "print(\"RNN\",rnn)\n",
    "\n",
    "h_seq, h_final = rnn(output,h_init)\n",
    "print(\"H_seq\",h_seq)\n",
    "print(\"H_final\",h_final)\n",
    "print(h_seq.size(),\"\\n\", h_final.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing - Linear layer - Integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear layer\n",
    "\n",
    "in_features = 4\n",
    "out_features = 5\n",
    "seq_length = 3\n",
    "batch_size = 2\n",
    "\n",
    "# RNN layer\n",
    "\n",
    "num_layers = 1\n",
    "num_directions = 1\n",
    "hidden_size = 5\n",
    "input_size = out_features\n",
    "\n",
    "# Linear layer\n",
    "linear = nn.Linear(out_features,in_features, bias = False)\n",
    "print(linear)\n",
    "print(linear.weight)\n",
    "\n",
    "# one hot encoding\n",
    "\n",
    "batch_1 = torch.LongTensor([1,2,3])\n",
    "batch_2 = torch.LongTensor([3,2,1])\n",
    "\n",
    "input_data = torch.cat((batch_1,batch_2),dim = 0).view(seq_length,batch_size)\n",
    "print(\"input_data\",input_data)\n",
    "print(\"Size of the input data\",input_data.size())\n",
    "\n",
    "output = linear.weight.data[input_data]\n",
    "print(\"Size of the Linear layer output\",output.size())\n",
    "#output = output.view(seq_length,batch_size,input_size)\n",
    "print(\"linear layer output\", output)\n",
    "\n",
    "# Memory state\n",
    "h_init = torch.zeros (num_layers * num_directions, batch_size, hidden_size)\n",
    "print(\"Memory state initialisation\", h_init)\n",
    "print(\"Memory state size\",h_init.size())\n",
    "\n",
    "rnn = nn.RNN(input_size,hidden_size,nonlinearity = 'tanh',bias = True)\n",
    "print(\"RNN\",rnn)\n",
    "\n",
    "h_seq, h_final = rnn(output,h_init)\n",
    "print(\"H_seq\",h_seq)\n",
    "print(\"H_final\",h_final)\n",
    "print(h_seq.size(),\"\\n\", h_final.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing - Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight Parameter containing:\n",
      "tensor([[-1.2872, -1.5725,  0.9193,  0.8244,  0.8569],\n",
      "        [ 0.1559,  0.9764, -1.3576,  1.1315, -1.7053],\n",
      "        [ 0.4271,  0.9379, -0.1343, -0.0158,  0.5403],\n",
      "        [-2.2810, -0.0063, -0.5422, -0.7179,  0.6794]], requires_grad=True)\n",
      "input torch.Size([3, 2])\n",
      "Size of the Embedding layer output torch.Size([3, 2, 5])\n",
      "Embedding layer output tensor([[[ 0.1559,  0.9764, -1.3576,  1.1315, -1.7053],\n",
      "         [-2.2810, -0.0063, -0.5422, -0.7179,  0.6794]],\n",
      "\n",
      "        [[ 0.4271,  0.9379, -0.1343, -0.0158,  0.5403],\n",
      "         [ 0.4271,  0.9379, -0.1343, -0.0158,  0.5403]],\n",
      "\n",
      "        [[-2.2810, -0.0063, -0.5422, -0.7179,  0.6794],\n",
      "         [ 0.1559,  0.9764, -1.3576,  1.1315, -1.7053]]],\n",
      "       grad_fn=<EmbeddingBackward>)\n",
      "Memory state initialisation tensor([[[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]]])\n",
      "Memory state size torch.Size([1, 2, 5])\n",
      "RNN RNN(5, 5)\n",
      "H_seq tensor([[[ 0.2328, -0.2686, -0.3293, -0.6375, -0.6561],\n",
      "         [ 0.8115, -0.1449, -0.1975, -0.6391,  0.1326]],\n",
      "\n",
      "        [[ 0.0413,  0.0969,  0.3130, -0.6775, -0.2612],\n",
      "         [ 0.2912,  0.3445,  0.2200, -0.5416, -0.1208]],\n",
      "\n",
      "        [[ 0.8014, -0.1443, -0.0218, -0.7244, -0.0560],\n",
      "         [ 0.3718, -0.3005, -0.2834, -0.6910, -0.7257]]],\n",
      "       grad_fn=<StackBackward>)\n",
      "H_final tensor([[[ 0.8014, -0.1443, -0.0218, -0.7244, -0.0560],\n",
      "         [ 0.3718, -0.3005, -0.2834, -0.6910, -0.7257]]],\n",
      "       grad_fn=<StackBackward>)\n",
      "torch.Size([3, 2, 5]) \n",
      " torch.Size([1, 2, 5])\n"
     ]
    }
   ],
   "source": [
    "# Linear layer\n",
    "\n",
    "in_features = 4\n",
    "out_features = 5\n",
    "seq_length = 3\n",
    "batch_size = 2\n",
    "\n",
    "# RNN layer\n",
    "\n",
    "num_layers = 1\n",
    "num_directions = 1\n",
    "hidden_size = 5\n",
    "input_size = out_features\n",
    "\n",
    "# Word embedding\n",
    "\n",
    "num_embeddings = 4\n",
    "embedding_dim = 5\n",
    "\n",
    "w_emb = nn.Embedding(num_embeddings, embedding_dim)\n",
    "print(\"weight\",w_emb.weight)\n",
    "\n",
    "input = torch.LongTensor([[1,2,3],[3,2,1]])\n",
    "input = torch.transpose(input,0,1)\n",
    "#print(\"input\",input.size())\n",
    "\n",
    "output = w_emb(input)\n",
    "print(\"Size of the Embedding layer output\",output.size())\n",
    "#output = output.view(seq_length,batch_size,input_size)\n",
    "print(\"Embedding layer output\", output)\n",
    "\n",
    "\n",
    "# Memory state\n",
    "h_init = torch.zeros (num_layers * num_directions, batch_size, hidden_size)\n",
    "print(\"Memory state initialisation\", h_init)\n",
    "print(\"Memory state size\",h_init.size())\n",
    "\n",
    "rnn = nn.RNN(input_size,hidden_size,nonlinearity = 'tanh',bias = True)\n",
    "print(\"RNN\",rnn)\n",
    "\n",
    "h_seq, h_final = rnn(output,h_init)\n",
    "print(\"H_seq\",h_seq)\n",
    "print(\"H_final\",h_final)\n",
    "print(h_seq.size(),\"\\n\", h_final.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear layer- one hot encoding - batch size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear layer\n",
    "\n",
    "in_features = 4\n",
    "out_features = 7\n",
    "seq_length = 3\n",
    "batch_size = 1\n",
    "\n",
    "# RNN layer\n",
    "\n",
    "num_layers = 1\n",
    "num_directions = 1\n",
    "hidden_size = 5\n",
    "input_size = out_features\n",
    "\n",
    "# Linear layer\n",
    "linear = nn.Linear(in_features, out_features, bias = False)\n",
    "print(linear)\n",
    "print(linear.weight)\n",
    "\n",
    "# one hot encoding\n",
    "\n",
    "batch_1 = torch.Tensor([[1,0,0,0],[0,0,1,0],[0,0,0,1]])\n",
    "#batch_2 = torch.Tensor([[0,0,1,0],[0,1,0,0],[1,0,0,0]])\n",
    "\n",
    "input_data = batch_1.view(seq_length,batch_size,in_features)\n",
    "print(\"input_data\",input_data)\n",
    "print(\"Size of the input data\",input_data.size())\n",
    "\n",
    "output = linear(input_data)\n",
    "print(\"Size of the Linear layer output\",output.size())\n",
    "#output = output.view(seq_length,batch_size,input_size)\n",
    "print(\"linear layer output\", output)\n",
    "\n",
    "# Memory state\n",
    "h_init = torch.zeros (num_layers * num_directions, batch_size, hidden_size)\n",
    "print(\"Memory state initialisation\", h_init)\n",
    "print(\"Memory state size\",h_init.size())\n",
    "\n",
    "rnn = nn.RNN(input_size,hidden_size,nonlinearity = 'tanh',bias = True)\n",
    "print(\"RNN\",rnn)\n",
    "\n",
    "h_seq, h_final = rnn(output,h_init)\n",
    "print(\"H_seq\",h_seq)\n",
    "print(\"H_final\",h_final)\n",
    "print(h_seq.size(),\"\\n\", h_final.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN score computation - one forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear layer\n",
    "\n",
    "in_features = 4\n",
    "out_features = 5\n",
    "seq_length = 3\n",
    "batch_size = 2\n",
    "\n",
    "# RNN layer\n",
    "\n",
    "num_layers = 1\n",
    "num_directions = 1\n",
    "hidden_size = 5\n",
    "input_size = out_features\n",
    "\n",
    "# Word embedding\n",
    "\n",
    "num_embeddings = 4\n",
    "embedding_dim = 5\n",
    "\n",
    "w_emb = nn.Embedding(num_embeddings, embedding_dim)\n",
    "print(\"weight\",w_emb.weight)\n",
    "\n",
    "input = torch.LongTensor([[1,2,3],[3,2,1]])\n",
    "input = torch.transpose(input,0,1)\n",
    "\n",
    "print(\"input\",input)\n",
    "\n",
    "labels = torch.LongTensor([[1,2,3],[3,2,1]])\n",
    "\n",
    "output = w_emb(input)\n",
    "print(\"Size of the Embedding layer output\",output.size())\n",
    "#output = output.view(seq_length,batch_size,input_size)\n",
    "print(\"Embedding layer output\", output)\n",
    "\n",
    "\n",
    "# Memory state\n",
    "h_init = torch.zeros (num_layers * num_directions, batch_size, hidden_size)\n",
    "print(\"Memory state initialisation\", h_init)\n",
    "print(\"Memory state size\",h_init.size())\n",
    "\n",
    "rnn = nn.RNN(input_size,hidden_size,nonlinearity = 'tanh',bias = True)\n",
    "print(\"RNN\",rnn)\n",
    "\n",
    "h_seq, h_final = rnn(output,h_init)\n",
    "print(\"H_seq\",h_seq)\n",
    "print(\"H_final\",h_final)\n",
    "print(h_seq.size(),\"\\n\", h_final.size())\n",
    "\n",
    "### Linear layer - for next word prediction \n",
    "\n",
    "score_layer = nn.Linear(hidden_size,num_embeddings)\n",
    "\n",
    "output_score = score_layer(h_seq).view(batch_size*seq_length,num_embeddings)\n",
    "print(output_score)\n",
    "\n",
    "labels = labels.view(batch_size*seq_length)\n",
    "print(labels)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "loss= criterion(output_score,labels)\n",
    "\n",
    "print(\"loss\",loss)\n",
    "\n",
    "#F = torch.softmax(output_score,dim=1)\n",
    "\n",
    "#print(F)\n",
    "#print(F[0].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear layer\n",
    "\n",
    "in_features = 4\n",
    "out_features = 5\n",
    "seq_length = 3\n",
    "batch_size = 2\n",
    "\n",
    "# RNN layer\n",
    "\n",
    "num_layers = 1\n",
    "num_directions = 2\n",
    "hidden_size = 5\n",
    "input_size = out_features\n",
    "\n",
    "# Word embedding\n",
    "\n",
    "num_embeddings = 4\n",
    "embedding_dim = 5\n",
    "\n",
    "w_emb = nn.Embedding(num_embeddings, embedding_dim)\n",
    "print(\"weight\",w_emb.weight)\n",
    "\n",
    "input = torch.LongTensor([[1,2,3],[3,2,1]])\n",
    "input = torch.transpose(input,0,1)\n",
    "print(\"input\",input)\n",
    "\n",
    "output = w_emb(input)\n",
    "print(\"Size of the Embedding layer output\",output.size())\n",
    "#output = output.view(seq_length,batch_size,input_size)\n",
    "print(\"Embedding layer output\", output)\n",
    "\n",
    "\n",
    "# Memory state\n",
    "h_init = torch.zeros (num_layers * num_directions, batch_size, hidden_size)\n",
    "print(\"Memory state initialisation\", h_init)\n",
    "print(\"Memory state size\",h_init.size())\n",
    "\n",
    "rnn = nn.RNN(input_size,hidden_size,nonlinearity = 'tanh',bias = True, bidirectional = True)\n",
    "print(\"RNN\",rnn)\n",
    "\n",
    "h_seq, h_final = rnn(output,h_init)\n",
    "print(\"H_seq\",h_seq)\n",
    "print(\"H_final\",h_final)\n",
    "print(h_seq.size(),\"\\n\", h_final.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
